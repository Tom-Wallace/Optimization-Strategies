# Optimization-Strategies

## Models

The full list of models tested is provided below:
### GPT2
https://huggingface.co/openai-community/gpt2
### GPT2-Large
https://huggingface.co/openai-community/gpt2-large
### GPT2-XL
https://huggingface.co/openai-community/gpt2-xl
### DistilGPT2
https://huggingface.co/distilbert/distilgpt2
### MiniLLM Repository
https://github.com/microsoft/LMOps/tree/main/minillm
Includes base, fine-tuned, KD, SeqKD, and MiniLLM-trained versions of GPT2-L, OPT-6.7B, OPT-2.7B, and Llama-6.7B.
### Llama-13B
https://huggingface.co/huggyllama/llama-13b
### Llama2-7B
https://huggingface.co/meta-llama/Llama-2-7b
### Sheared-Llama
https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B
https://huggingface.co/princeton-nlp/Sheared-LLaMA-2.7B
### Mistral-NeMo-Base 12B
https://huggingface.co/mistralai/Mistral-Nemo-Base-2407
### Mistral-NeMo-Minitron
https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base
### Llama3.1-8B
https://huggingface.co/meta-llama/Llama-3.1-8B
### Llama-3.1-Minitron (Width)
https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Width-Base

## Tables

Tables showing experiment results that are not listed in the paper itself are [available in this file.](Set_1_Extra_Tables.pdf)
